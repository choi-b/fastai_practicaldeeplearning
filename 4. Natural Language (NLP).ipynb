{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8466ed80",
   "metadata": {},
   "source": [
    "# Notes: Hugging Face Transformers (won't use fastai library for this lesson)\n",
    "\n",
    "* following the lesson, we will fine-tune a pre-trained NLP model using a library called HuggingFace Transformer\n",
    "* reason: \n",
    "    * really useful to get experience using other libraries (good for reinforcing knowledge)\n",
    "    * hugging face is really good for NLP, well worth knowing\n",
    "    * probably will have finished integration of transformer library in fastai\n",
    "    \n",
    "* **Hugging Transformer** doesn't have the same architecture as fastai\n",
    "    * lower level, will need to do a bit more work on our end\n",
    "\n",
    "* **Pre-trained model** - a bunch of parameters already fit, some of them - already confident what they should be, some of them - no idea what they should be at all. Hence the need for fine-tuning.\n",
    "\n",
    "* **ULMFiT** - an architecture and transfer learning method that can be applied to NLP tasks\n",
    "    * Started out on Wikipedia data to predict the next word (got up to ~30% accuracy)\n",
    "    * Then applied to IMDB data, took the pre-trained model on wikipedia and ran a few more epochs, then took those weights and fine tuned them to classify a review as positive or negative\n",
    "\n",
    "* Used RNNs around the same time.\n",
    "* **Transformers** - took good really advantage of modern accelerators like Google TPUs\n",
    "    * Threw away the idea of predicting the next word of a sentence\n",
    "    * Took chunks of wikipedia, deleted at random a few words, asked the model which words were deleted\n",
    "\n",
    "* For this lesson, we'll focus on the **Transformers masked language model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed268ee4",
   "metadata": {},
   "source": [
    "## Reference: Kaggle Competition - Getting Started with NLP for absolute beginners (U.S. patent phrase to phrase matching)\n",
    "\n",
    "* Data:\n",
    "    * id, anchor, target, context, score (how similar target & anchor are)\n",
    "    \n",
    "* [Link to competition](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb15423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nlp_intro/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c427b2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8917989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object') #Not that much language data, lots of repeated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f920251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can represent the input to the model as for example, \"TEXT1: abatement; TEXT2: eliminating process\" \n",
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f83732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a7c03",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "* Neural networks work with numbers\n",
    "\n",
    "* Step 1: Split these into tokens (words)\n",
    "    * Unique words will get a number\n",
    "    * Generally, don't want a vocabulary to be too big\n",
    "    * Nowadays, people use subwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e55a6",
   "metadata": {},
   "source": [
    "Transformers uses a **Dataset object** for storing a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b386da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2915e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5165aa2",
   "metadata": {},
   "source": [
    "### Numericalization\n",
    "* Step 2: Convert each word (or token) into a number. \"Unique ID\" based on the vocabulary\n",
    "* Details about how Step 1 & 2 are done -> depend on the particular model we use. (Hugging face has [250K+ models](https://huggingface.co/models) as of July 2023)\n",
    "    * A reasonable starting point is to use \"deberta-v3-small\"\n",
    "    * start with small, then explore large for slower but more accurate results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f90773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the model here\n",
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e07e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#AutoTokenizer creates a tokenizer appropriate for this model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffba22cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Yo', '▁what', '▁up', ',', '▁this', '▁is', '▁Link', '▁from', '▁Hyrule']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try passing a string to this tokenizer\n",
    "tokz.tokenize(\"Yo what up, this is Link from Hyrule\")\n",
    "#Underscore indicates the START of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338ed6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁A',\n",
       " '▁platypus',\n",
       " '▁is',\n",
       " '▁an',\n",
       " '▁or',\n",
       " 'ni',\n",
       " 'tho',\n",
       " 'rhynch',\n",
       " 'us',\n",
       " '▁an',\n",
       " 'at',\n",
       " 'inus',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#less common phrase\n",
    "tokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021851c",
   "metadata": {},
   "source": [
    "#### Create a simple function to tokenize our inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d66204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8b2cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run this quickly in parallel using 'map'\n",
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "002fbb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n",
       " [1,\n",
       "  54453,\n",
       "  435,\n",
       "  294,\n",
       "  336,\n",
       "  5753,\n",
       "  346,\n",
       "  54453,\n",
       "  445,\n",
       "  294,\n",
       "  47284,\n",
       "  265,\n",
       "  6435,\n",
       "  346,\n",
       "  23702,\n",
       "  435,\n",
       "  294,\n",
       "  47284,\n",
       "  2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the first row of the tokenizer\n",
    "row = tok_ds[0]\n",
    "row['input'], row['input_ids'] #Successfully turned our tokens into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4089145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try looking up words in a dictionary to get the numbers\n",
    "tokz.vocab['▁of'] \n",
    "#To handle the whitespace as a basic token explicitly, \n",
    "#SentencePiece first escapes the whitespace with a meta symbol “▁” (U+2581) as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25c023",
   "metadata": {},
   "source": [
    "* ULMFiT: probably best for reasonably quick and easy implementation for long documents\n",
    "* Transformers: large documents are challenging, specifically since transformers have to do all documents at once (larger GPU cost)\n",
    "* Example: documents of over 2000 words? consider ULMFiT\n",
    "\n",
    "#### HuggingFace transformers expects that your target is called 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35d34e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the score column to labels\n",
    "tok_ds = tok_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023dd9",
   "metadata": {},
   "source": [
    "#### Test and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56233493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa2373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752151c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab907a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7457ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74302db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
