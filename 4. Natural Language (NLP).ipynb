{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8466ed80",
   "metadata": {},
   "source": [
    "# Notes: Hugging Face Transformers (won't use fastai library for this lesson)\n",
    "\n",
    "* following the lesson, we will fine-tune a pre-trained NLP model using a library called HuggingFace Transformer\n",
    "* reason: \n",
    "    * really useful to get experience using other libraries (good for reinforcing knowledge)\n",
    "    * hugging face is really good for NLP, well worth knowing\n",
    "    * probably will have finished integration of transformer library in fastai\n",
    "    \n",
    "* **Hugging Transformer** doesn't have the same architecture as fastai\n",
    "    * lower level, will need to do a bit more work on our end\n",
    "\n",
    "* **Pre-trained model** - a bunch of parameters already fit, some of them - already confident what they should be, some of them - no idea what they should be at all. Hence the need for fine-tuning.\n",
    "\n",
    "* **ULMFiT** - an architecture and transfer learning method that can be applied to NLP tasks\n",
    "    * Started out on Wikipedia data to predict the next word (got up to ~30% accuracy)\n",
    "    * Then applied to IMDB data, took the pre-trained model on wikipedia and ran a few more epochs, then took those weights and fine tuned them to classify a review as positive or negative\n",
    "\n",
    "* Used RNNs around the same time.\n",
    "* **Transformers** - took good really advantage of modern accelerators like Google TPUs\n",
    "    * Threw away the idea of predicting the next word of a sentence\n",
    "    * Took chunks of wikipedia, deleted at random a few words, asked the model which words were deleted\n",
    "\n",
    "* For this lesson, we'll focus on the **Transformers masked language model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed268ee4",
   "metadata": {},
   "source": [
    "## Reference: Kaggle Competition - Getting Started with NLP for absolute beginners (U.S. patent phrase to phrase matching)\n",
    "\n",
    "* Data:\n",
    "    * id, anchor, target, context, score (how similar target & anchor are)\n",
    "    \n",
    "* [Link to competition](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data?select=train.csv)\n",
    "\n",
    "* In this exercise, we're tasked with comparing two words or phrases, and scoring them based on whether they're similar or not, based on which patent class they were used in.\n",
    "\n",
    "* Goal of the competition: come up with a model that auto determines which \"anchor\" and \"target\" pairs are talking about the same thing\n",
    "\n",
    "* Score of 1: means the two inputs have identical meaning, 0 means they have totally different meaning. Score can be between 0 and 1.\n",
    "\n",
    "* This can be represented as a classification problem. Ex) For the following text...: \"TEXT1: abatement; TEXT2: eliminating process\" ...chose a category of meaning similarity: \"Different; Similar; Identical\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb15423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nlp_intro/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a08351",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('nlp_intro/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c427b2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8917989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object') #Not that much language data, lots of repeated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f920251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can represent the input to the model as for example, \"TEXT1: abatement; TEXT2: eliminating process\" \n",
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f83732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a7c03",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "* Neural networks work with numbers\n",
    "\n",
    "* Step 1: Split these into tokens (words)\n",
    "    * Unique words will get a number\n",
    "    * Generally, don't want a vocabulary to be too big\n",
    "    * Nowadays, people use subwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e55a6",
   "metadata": {},
   "source": [
    "Transformers uses a **Dataset object** for storing a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b386da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2915e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5165aa2",
   "metadata": {},
   "source": [
    "### Numericalization\n",
    "* Step 2: Convert each word (or token) into a number. \"Unique ID\" based on the vocabulary\n",
    "* Details about how Step 1 & 2 are done -> depend on the particular model we use. (Hugging face has [250K+ models](https://huggingface.co/models) as of July 2023)\n",
    "    * A reasonable starting point is to use \"deberta-v3-small\"\n",
    "    * start with small, then explore large for slower but more accurate results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f90773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the model here\n",
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e07e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#AutoTokenizer creates a tokenizer appropriate for this model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffba22cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Yo', '▁what', '▁up', ',', '▁this', '▁is', '▁Link', '▁from', '▁Hyrule']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try passing a string to this tokenizer\n",
    "tokz.tokenize(\"Yo what up, this is Link from Hyrule\")\n",
    "#Underscore indicates the START of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "338ed6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁A',\n",
       " '▁platypus',\n",
       " '▁is',\n",
       " '▁an',\n",
       " '▁or',\n",
       " 'ni',\n",
       " 'tho',\n",
       " 'rhynch',\n",
       " 'us',\n",
       " '▁an',\n",
       " 'at',\n",
       " 'inus',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#less common phrase\n",
    "tokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021851c",
   "metadata": {},
   "source": [
    "#### Create a simple function to tokenize our inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d66204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d8b2cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run this quickly in parallel using 'map'\n",
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002fbb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n",
       " [1,\n",
       "  54453,\n",
       "  435,\n",
       "  294,\n",
       "  336,\n",
       "  5753,\n",
       "  346,\n",
       "  54453,\n",
       "  445,\n",
       "  294,\n",
       "  47284,\n",
       "  265,\n",
       "  6435,\n",
       "  346,\n",
       "  23702,\n",
       "  435,\n",
       "  294,\n",
       "  47284,\n",
       "  2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the first row of the tokenizer\n",
    "row = tok_ds[0]\n",
    "row['input'], row['input_ids'] #Successfully turned our tokens into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4089145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try looking up words in a dictionary to get the numbers\n",
    "tokz.vocab['▁of'] \n",
    "#To handle the whitespace as a basic token explicitly, \n",
    "#SentencePiece first escapes the whitespace with a meta symbol “▁” (U+2581) as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25c023",
   "metadata": {},
   "source": [
    "* ULMFiT: probably best for reasonably quick and easy implementation for long documents\n",
    "* Transformers: large documents are challenging, specifically since transformers have to do all documents at once (larger GPU cost)\n",
    "* Example: documents of over 2000 words? consider ULMFiT\n",
    "\n",
    "#### HuggingFace transformers expects that your target is called 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d34e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the score column to labels\n",
    "tok_ds = tok_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a1996",
   "metadata": {},
   "source": [
    "* Create a validation set: tells us whether our models are underfit/overfit, etc\n",
    "* If you use the fastai library, it auto creates a validation set for you if you don't have one\n",
    "* Transformers use a DatasetDict for holding your training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aaa2373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 27354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#25% validation, 75% training\n",
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "dds #**notice, the validation set here is called 'test' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752151c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test dataset: accuracy of your model on test set is only checked after completing the entire training process.\n",
    "#Use \"eval\" as the name for the test set, to avoid confusion with the 'test' dataset that was created above\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c393ed",
   "metadata": {},
   "source": [
    "#### Transformers expects metrics to be returned as a dict. This way, the trainer knows what labels to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab907a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to do that\n",
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e479656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will use this funciton later: returns a single # we need given a pair of variables\n",
    "def corr(x,y): return np.corrcoef(x,y)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83354f5b",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74302db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e60af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a batch size that fits our GPU and a small # of epochs to run the experiments quickly\n",
    "bs = 128\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4dd9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most important hyperparameter: learning rate. \n",
    "#Fastai provides a learning rate finder to help you figure it out, but Transformers doesn't.\n",
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d61212cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformers use \"TrainingArguments\" to set up arguments.\n",
    "#These standard values generally work fine in most cases\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e9621",
   "metadata": {},
   "source": [
    "#### Create a model and \"Trainer\", which is a class that combines the data and model together (just like Learner in fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb02bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Transformers spits out lots of warnings, but you can ignore them.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=corr_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68bea5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [856/856 06:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.025546</td>\n",
       "      <td>0.799359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024128</td>\n",
       "      <td>0.821532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.833313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.835018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train our model - again lots of warnings - can ignore them\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd8061",
   "metadata": {},
   "source": [
    "#### Key thing to notice is the \"Pearson\" value in the table above.\n",
    "* It's increasing and is already above 0.8 (great news).\n",
    "* On Kaggle, submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
